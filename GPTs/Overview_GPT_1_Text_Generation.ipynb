{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wk8SZPlyT8N"
      },
      "source": [
        "# Overview of GPT-1 on Text_Generation\n",
        "\n",
        "## Decoder only Transformer\n",
        "`Input:` A prompt(often referred to as a context) fed into the transformer  as a whole.\n",
        "\n",
        "`Output:` It depends on the goal of the model for GPT models, the output is the probability distribution of the next token/word that comes after the prompt. It outputs the one prediction for the complete input.\n",
        "\n",
        "\n",
        "![bWnx0](https://github.com/surajkarki66/MediLeaf_backend/assets/50628520/6b9f187e-73ea-4c69-81ff-723ba211c934)\n",
        "\n",
        "### 1. Self-attention Mechanism\n",
        "It allows model to focus  on the most relevant parts of the input. A single self attention mechanism is called head.\n",
        "\n",
        "![wH0ra](https://github.com/surajkarki66/MediLeaf_backend/assets/50628520/cbc885bc-9ba0-4a0d-ac98-ddf940ebb3a9)\n",
        "\n",
        "\n",
        "### 2. Training\n",
        "#### 2a. Basic Training\n",
        "The basic training process consist of self supervised learning. Simply put, you gather lots of text, strip the last word from that text, feed it as input into the transformer, check if the prediction matches the word you cut off and backpropagate the error.\n",
        "\n",
        "For example, data = \"This is a sample\"  \n",
        "sample = [\n",
        "  [\"This\"],\n",
        "  [\"This\", \"is\"],\n",
        "  [\"This\", \"is\", \"a\"]\n",
        "]  \n",
        "targets = [\"is\", \"a\", \"sample\"]\n",
        "\n",
        "#### 2b. Fine-tuning\n",
        "After the first stage of training is completed, the model is now a large language model. However, through fine tuning the model can be adapted to better suit the needs of the final application. One of the key reasons why ChatGPT and GPT4 seem so ridiculously impressive is because of this second stage of training.\n",
        "\n",
        "#### 2c. Fine-tuning\n",
        "Doing inference with a transformer is just like training. You insert a prompt and out comes the next word. For GPT models, this means that the prompt is extended one word at a time. You insert the prompt, and outcomes the first word of the answer. The first word of answer  is now added to the prompt, creating a new slightly different prompt. This prompt is again forwarded through the model, giving the prediction of new word.\n",
        "\n",
        "\n",
        "GPT-1, or Generative Pre-trained Transformer 1, is a state-of-the-art natural language processing model developed by OpenAI. It's part of the Transformer architecture, a type of neural network known for its ability to process sequential data, such as text, very effectively. GPT-2 specifically is designed for text generation tasks, where it predicts the likelihood of a sequence of words given some input context.\n",
        ":"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPT-1\n",
        "The architecture of GPT-1 is given below:\n",
        "\n",
        "<img width=\"224\" alt=\"GPT-architecture\" src=\"https://github.com/surajkarki66/MediLeaf_backend/assets/50628520/433647de-74f8-48ee-9f3c-7eb54cd385fc\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key Ideas**\n",
        "- Stack a bunch of Transformer decoders.\n",
        "- Self supervised pre-training helps a lot.\n",
        "- Fine-tune on multiple tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img width=\"993\" alt=\"training\" src=\"https://github.com/surajkarki66/MediLeaf_backend/assets/50628520/4c8227ea-ad0d-4107-aa9c-52e63bbc1b31\">\n",
        "\n",
        "The GPT-1 model is first pre-trained on texts from 7000 books and then further fine tuning for the following tasks:\n",
        "- Textual Entailment\n",
        "- Question Answering\n",
        "- Semantic Similarity\n",
        "- Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training\n",
        "#### 1. Unsupervised Pre-training\n",
        "Given an unsupervised corpus of tokens $u = \\{u_1, u_2, \\ldots, u_n\\}$, we use standard language modeling objective to maximize the following likelihood:\n",
        "\n",
        "![image](https://github.com/surajkarki66/MediLeaf_backend/assets/50628520/b74617c3-53b5-48e4-99a9-81d95ef0bfbf)\n",
        "\n",
        "where k is the size of the context window, and the conditional probability P is modeled using a neural\n",
        "network with parameters Î˜. These parameters are trained using stochastic gradient descent.\n",
        "\n",
        "#### 2. Supervised fine-tuning\n",
        "After training the model with the objective in above equation, we adapt the parameters to the supervised target task.\n",
        "\n",
        "This gives us the following objective:\n",
        "![image](https://github.com/surajkarki66/MediLeaf_backend/assets/50628520/28fcb7cd-729d-4ebc-921e-96d1e94ddb2d)\n",
        "\n",
        "where C is labeled dataset, y is label and $\\{x^1, x^2, \\ldots, x^m\\}$ is the sequence of input tokens.\n",
        "\n",
        "Final, auxillary objective is \n",
        "![image](https://github.com/surajkarki66/MediLeaf_backend/assets/50628520/0302215b-d86d-4bfc-a98b-2f332051aac9)\n",
        "\n",
        "- This auxillary function function improves generalization of supervised model\n",
        "- Accelerating convergence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tasks\n",
        "### 1. Textual Entailment\n",
        "\n",
        "| Premise       | Hypothesis   | Label         |\n",
        "|---------------|--------------|---------------|\n",
        "| Adam sleeps   | Adam snores  | Entailment    |\n",
        "| Adam sleeps   | Adam codes   | Contradiction |\n",
        "| Adam sleeps   | Michael Sings| Neutral       |\n",
        "\n",
        "![image](https://github.com/surajkarki66/MediLeaf_backend/assets/50628520/893462ad-31ea-4b4c-9f97-d2cd85b5ad31)\n",
        "\n",
        "\n",
        "### 2. Question Answering\n",
        "| Text       | Question   | Score(similarity)         |\n",
        "|---------------|--------------|---------------|\n",
        "| Article   | About article  | value    |\n",
        "\n",
        "![image](https://github.com/surajkarki66/MediLeaf_backend/assets/50628520/3dfcdec9-c6f3-4bff-838a-07d1298e0e46)\n",
        "\n",
        "\n",
        "### 3. Semantic Similarity\n",
        "| Text 1       | Text 2   | Label       |\n",
        "|---------------|--------------|---------------|\n",
        "| What can do after MBBS?   | What do I do after MBBS  | Duplicate    |\n",
        "\n",
        "![image](https://github.com/surajkarki66/MediLeaf_backend/assets/50628520/223e20ab-d9ac-42fd-b261-42bdd5a8b1ca)\n",
        "\n",
        "\n",
        "### 4. Classification\n",
        "![image](https://github.com/surajkarki66/MediLeaf_backend/assets/50628520/cb69e38b-d459-4b64-a173-29fc1c7b96e2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Some Notes Regarding GPT-1**  \n",
        "- Uses learned position embedding.\n",
        "- Activation function => Gaussian Error Linear Unit\n",
        "- L2 regularization proposed with w=0.01\n",
        "- Learning rate for fine tuning = 6.25e-5\n",
        "- Batch size for fine-tuning =32\n",
        "- Epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AXeMp6jzVBQ"
      },
      "source": [
        "Let's play little bit with GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsSSSHSUzZfD",
        "outputId": "ccc16584-3f7a-4b0a-c11a-fb31b12a6611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HPYulqAnkVfK"
      },
      "outputs": [],
      "source": [
        "from transformers import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADMJU-G8kh3a"
      },
      "source": [
        "## 1. Convert the sentences into the tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hicw4gDZkfWY",
        "outputId": "6110b306-7fa0-431f-e726-ca87d41223e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAIGPTTokenizer(name_or_path='openai-gpt', vocab_size=40478, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '<unk>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
        "print(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f5d732bbb415495ea29933f04c02184a",
            "0de93ade2afb45d1a323e1972b2c7b7f",
            "7e740342579e4d8f879fe52de149160e",
            "ab27ce2f439848069de99ea8839ca3fd",
            "054a40d18f5c42b3862ba928b3fb8f9b",
            "0625e34d742947b689689e0bba466ed1",
            "e40f583d0c47442aa20a9155a8d34bd4",
            "795a831315174cfb85e1cb63eb5d312e",
            "3fc03c0fd93a4af1a8cd0a80dd1a3350",
            "3256b271aae247c295726502e92a87f9",
            "56f4a0573af5455fb0f2c931b935c250"
          ]
        },
        "id": "N_KFSoGMlNAg",
        "outputId": "f12591e1-d8d3-4f86-9b0d-c8dce689602a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5d732bbb415495ea29933f04c02184a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt', pad_token_id=tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "crDoqR4ClsjA"
      },
      "outputs": [],
      "source": [
        "sentence = 'ChatGPT should be open sourced.'\n",
        "numeric_ids = tokenizer.encode(sentence, return_tensors = 'pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQIdC9s9l4GS",
        "outputId": "3ffbb060-ee32-4c4c-81e6-7b9e91cc385b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 879,    3,   30, 1992,  994,  580, 1189, 3586,  940,  239]])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "numeric_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Z_UrBiy-l_tf",
        "outputId": "585b596d-08cc-40a4-9b15-e2ea77923f99"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'chatgpt should be open sourced.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(numeric_ids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBHL_cB9mWrw"
      },
      "source": [
        "## 2. Generate the text given the sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "t-l8Qy3amLMC"
      },
      "outputs": [],
      "source": [
        "result = model.generate(numeric_ids, max_length = 100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CFdiOp3mjuP",
        "outputId": "24c3f5f2-f959-4496-bba7-4fa413a20412"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  879,     3,    30,  1992,   994,   580,  1189,  3586,   940,   239,\n",
              "           244, 40477,   244,   249,   587,   538,   825,   620,   240,   244,\n",
              "           520,   603,   239, 40477,   487,   816,   491,   513,   562,   246,\n",
              "           928,  1113,   240,   488,   674,   487,   603,   240,   500,   246,\n",
              "          1002,   620,   954,   520,   558,   485,  9261,   485,  1344,   575,\n",
              "           715,   481,  1465,   498,   481,  4406,   240, 40477,   256,   256,\n",
              "           249,   825,   512,   761,   770,   239,   249,   719,   797,   485,\n",
              "           604,   485,   587,   846,   670,   525,   239,  6725, 40477,   520,\n",
              "           816,   609,   491,   575,   240,   513,   741,  2144,   239,   487,\n",
              "           509,  2538,   239,   507,   509,   481,  1242,   498,   246,   762]])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFGPRIm0mkhn",
        "outputId": "81cc6dd9-a349-4edf-db87-4b95783a0137"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "chatgpt should be open sourced. \" \n",
            " \" i don't think so, \" she said. \n",
            " he looked at her for a long moment, and then he said, in a voice so low she had to strain to hear him over the sound of the engine, \n",
            "'' i think you're right. i'm going to have to do something about that. '' \n",
            " she looked up at him, her eyes wide. he was smiling. it was the smile of a man\n"
          ]
        }
      ],
      "source": [
        "generated_text = tokenizer.decode(result[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "8hSKFO00nuif"
      },
      "outputs": [],
      "source": [
        "sentence2 = 'Suraj loves sandhya and'\n",
        "numeric_ids2 = tokenizer.encode(sentence2, return_tensors = 'pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "xsdeeLFI3ACK"
      },
      "outputs": [],
      "source": [
        "result2 = model.generate(numeric_ids2, max_length = 100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4L8mstn3HOe",
        "outputId": "747fd42b-80ba-4fe6-9bfa-51436274a89b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "suraj loves sandhya and wants to be a part of it.'\n",
            "'i don't know what to say to that,'suraj said. \n",
            " saira looked at him with a serious face. she didn't want to talk about her father's death. it was not something she wanted to discuss with suraj. he was the only person she could talk to about it, and she wasn't sure if she should tell him the whole story of her life. the last thing she needed was to\n"
          ]
        }
      ],
      "source": [
        "generated_text = tokenizer.decode(result2[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTuZFm343Mqz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "054a40d18f5c42b3862ba928b3fb8f9b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0625e34d742947b689689e0bba466ed1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0de93ade2afb45d1a323e1972b2c7b7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0625e34d742947b689689e0bba466ed1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e40f583d0c47442aa20a9155a8d34bd4",
            "value": "generation_config.json:â€‡100%"
          }
        },
        "3256b271aae247c295726502e92a87f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fc03c0fd93a4af1a8cd0a80dd1a3350": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "56f4a0573af5455fb0f2c931b935c250": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "795a831315174cfb85e1cb63eb5d312e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e740342579e4d8f879fe52de149160e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_795a831315174cfb85e1cb63eb5d312e",
            "max": 74,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3fc03c0fd93a4af1a8cd0a80dd1a3350",
            "value": 74
          }
        },
        "ab27ce2f439848069de99ea8839ca3fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3256b271aae247c295726502e92a87f9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_56f4a0573af5455fb0f2c931b935c250",
            "value": "â€‡74.0/74.0â€‡[00:00&lt;00:00,â€‡1.49kB/s]"
          }
        },
        "e40f583d0c47442aa20a9155a8d34bd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5d732bbb415495ea29933f04c02184a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0de93ade2afb45d1a323e1972b2c7b7f",
              "IPY_MODEL_7e740342579e4d8f879fe52de149160e",
              "IPY_MODEL_ab27ce2f439848069de99ea8839ca3fd"
            ],
            "layout": "IPY_MODEL_054a40d18f5c42b3862ba928b3fb8f9b"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
