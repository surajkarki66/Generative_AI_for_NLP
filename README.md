# Generative_AI_for_NLP
## Overview
Welcome to the repository that explores Generative Artificial Intelligence (AI) techniques in Natural Language Processing (NLP). This repository is a central hub for various research and works on generating and understanding human-like text using AI models.

## About
Generative AI has revolutionized Natural Language Processing by enabling machines to generate coherent and contextually relevant text. From language generation to translation, summarization, and dialogue systems, generative models have demonstrated remarkable capabilities in understanding and producing human-like language. This repository aims to showcase different approaches, and algorithms of Generative AI specifically tailored to the domain of NLP.

## Techniques
### 1. Transformers
### 2. Transformer based Architecture
#### 2a. Encoder-Decoder Architecture  
- The Encoder-Decoder architecture was the original transformer architecture introduced in the Attention Is All You Need paper.
- It works as follows: the encoder (on the left) processes the input sequence and generates a hidden representation that summarizes the input information. The decoder (on the right) uses this hidden representation to generate the desired output sequence.
- Suitable for Language Translation, Text Summarization, Question Answering, etc.
- Example models using this architecture are:
    - T5 â€“ [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683)
    - BART: [Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)
    - Longformer: [The Long-Document Transformer](https://arxiv.org/pdf/2004.05150)

#### 2b. Encoder only Architecture  




### 3. Large Language Models
### 4. Vector Databases
### 5. Many More

## Contact
For questions, suggestions, or collaborations, feel free to reach out to the repository owner [suraj.karki500@gmail.com](mailto:suraj.karki500@gmail.com)

Happy Coding!
