{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"02e583abe9934bb293f83fb570a41275":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8d3924217354428b989ec29cb9af4399","IPY_MODEL_e238320d7eb64706a6936a088afe598c","IPY_MODEL_dca2b10994e24b1c88376338d2cbd4b7"],"layout":"IPY_MODEL_43de22dcee5d4bc1a63e2c42d961b9a8"}},"8d3924217354428b989ec29cb9af4399":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_76476a926939422b9b6d925bbf3f8b6c","placeholder":"​","style":"IPY_MODEL_4491e869843843e7b7f3c377c8072a57","value":"Downloading readme: 100%"}},"e238320d7eb64706a6936a088afe598c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e38f163f771347fc9c6379276c6e142c","max":25164,"min":0,"orientation":"horizontal","style":"IPY_MODEL_08bda106016b44d88e9f7af7cc1a182a","value":25164}},"dca2b10994e24b1c88376338d2cbd4b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ad39cc79f6941c2a436ee71ca7a9329","placeholder":"​","style":"IPY_MODEL_ddac8fd72a964167882c180184142dd7","value":" 25.2k/25.2k [00:00&lt;00:00, 929kB/s]"}},"43de22dcee5d4bc1a63e2c42d961b9a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76476a926939422b9b6d925bbf3f8b6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4491e869843843e7b7f3c377c8072a57":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e38f163f771347fc9c6379276c6e142c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08bda106016b44d88e9f7af7cc1a182a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5ad39cc79f6941c2a436ee71ca7a9329":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddac8fd72a964167882c180184142dd7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5a5cf93c75344dd899ba60b4118909c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aa386c329e3a4fd8b77e13667c6f2c16","IPY_MODEL_fe97f0efd0754e0bb6944dbbc8377179","IPY_MODEL_a85d730452674578a7917e5b92afee30"],"layout":"IPY_MODEL_1b40e8dc869b41f5a6a830438fc31b16"}},"aa386c329e3a4fd8b77e13667c6f2c16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9334a9b9ebcf41b49405063f7a0a194a","placeholder":"​","style":"IPY_MODEL_d6d1f16ef9c1405b952d8ab59de43597","value":"Downloading data: 100%"}},"fe97f0efd0754e0bb6944dbbc8377179":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1b2269252544c8d96437b167f669d24","max":5726189,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b758743e82a047c0bdf879cc298e6e00","value":5726189}},"a85d730452674578a7917e5b92afee30":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bba8656bc667482f8ee877a5f19de8a0","placeholder":"​","style":"IPY_MODEL_9f16a6ec3a4348dd88794dde337f6063","value":" 5.73M/5.73M [00:00&lt;00:00, 11.9MB/s]"}},"1b40e8dc869b41f5a6a830438fc31b16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9334a9b9ebcf41b49405063f7a0a194a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6d1f16ef9c1405b952d8ab59de43597":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1b2269252544c8d96437b167f669d24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b758743e82a047c0bdf879cc298e6e00":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bba8656bc667482f8ee877a5f19de8a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f16a6ec3a4348dd88794dde337f6063":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"22ea7eff40f74dbfa694522241f7f24e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_789057a49a094522b1d68bc592c0d623","IPY_MODEL_bb5d6857e05f42ff935804c407387f53","IPY_MODEL_7fc9814e7c1640d2add5da1f469e22ed"],"layout":"IPY_MODEL_a12390ff676642fa86bf0ad0b8d6531b"}},"789057a49a094522b1d68bc592c0d623":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e6d8215175c4ea9ad598fd4802a3e10","placeholder":"​","style":"IPY_MODEL_6b611b7018564e44b01d938e2ca50da8","value":"Generating train split: 100%"}},"bb5d6857e05f42ff935804c407387f53":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a954069ddf2497a9a676adccbad9148","max":32332,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a15dd419a494458ab45bd149759c651f","value":32332}},"7fc9814e7c1640d2add5da1f469e22ed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f6dafac0a644042865c293a7910a712","placeholder":"​","style":"IPY_MODEL_14a8b7b60971416da3c1a5b8947647c6","value":" 32332/32332 [00:00&lt;00:00, 167419.57 examples/s]"}},"a12390ff676642fa86bf0ad0b8d6531b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e6d8215175c4ea9ad598fd4802a3e10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b611b7018564e44b01d938e2ca50da8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a954069ddf2497a9a676adccbad9148":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a15dd419a494458ab45bd149759c651f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1f6dafac0a644042865c293a7910a712":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14a8b7b60971416da3c1a5b8947647c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine tune T5 on Question Answering\n\n## T5\n- It is a text to text transfer transformer. \n- T5 alone can be used to perform different NLP taska such as Text classification, language translation, text summarization, question answering makes itself a most flexible model.","metadata":{}},{"cell_type":"markdown","source":"**What it does?**  \n1. Convert all problems to text to text generation.\n    - For example: In Language Translation, English to Italian  \n    Input: I love you  \n    Output: Ti amo\n    \n    - For example: In Text Classification,  \n    Input: This product is trash.  \n    Output: Negative\n\n2. Learns to predict [MASK] words.\n3. Use task specific prefixes to guide the model during fine tuning.\n   For example, it adds specific token at the beginning of the input text to indicate what  task is it performing.","metadata":{}},{"cell_type":"markdown","source":"T5 has been shown to achieve state-of-the-art results on a wide range of NLP tasks, and it’s considered a highly sophisticated and powerful NLP model, showing a high level of versatility, fine-tuning capability, and an efficient way to transfer knowledge.","metadata":{}},{"cell_type":"markdown","source":"## Implementation\n","metadata":{}},{"cell_type":"markdown","source":"### 1. Installation","metadata":{}},{"cell_type":"code","source":"!pip install transformers\n!pip install evaluate\n!pip install rouge","metadata":{"execution":{"iopub.status.busy":"2024-02-18T16:57:05.911920Z","iopub.execute_input":"2024-02-18T16:57:05.912379Z","iopub.status.idle":"2024-02-18T16:57:43.571255Z","shell.execute_reply.started":"2024-02-18T16:57:05.912349Z","shell.execute_reply":"2024-02-18T16:57:43.570110Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.37.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nCollecting evaluate\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.24.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.12.2)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.20.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\nCollecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge) (1.16.0)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport json\nimport torch.nn as nn\nimport nltk\nimport spacy\nimport string\nimport evaluate  # Bleu\nimport pandas as pd\nimport numpy as np\nimport transformers\nimport matplotlib.pyplot as plt\nimport warnings\n\n\nfrom tqdm import tqdm\nfrom torch.optim import Adam\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler\nfrom transformers import T5Tokenizer, T5Model, T5ForConditionalGeneration, T5TokenizerFast\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"id":"NMwbh8_Vy8Uq","execution":{"iopub.status.busy":"2024-02-18T16:57:43.574940Z","iopub.execute_input":"2024-02-18T16:57:43.575294Z","iopub.status.idle":"2024-02-18T16:58:06.759501Z","shell.execute_reply.started":"2024-02-18T16:57:43.575260Z","shell.execute_reply":"2024-02-18T16:58:06.758698Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2024-02-18 16:57:55.461186: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-18 16:57:55.461283: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-18 16:57:55.589790: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 2. Setup models","metadata":{}},{"cell_type":"code","source":"DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nTOKENIZER = T5TokenizerFast.from_pretrained(\"t5-base\")\nMODEL = T5ForConditionalGeneration.from_pretrained(\"t5-base\", return_dict=True).to(DEVICE)\nOPTIMIZER = Adam(MODEL.parameters(), lr=0.00001)\nQ_LEN = 256   # Question Length\nT_LEN = 32    # Target Length\nBATCH_SIZE = 16","metadata":{"execution":{"iopub.status.busy":"2024-02-18T16:58:06.760656Z","iopub.execute_input":"2024-02-18T16:58:06.761440Z","iopub.status.idle":"2024-02-18T16:58:13.590084Z","shell.execute_reply.started":"2024-02-18T16:58:06.761411Z","shell.execute_reply":"2024-02-18T16:58:13.588859Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85c28a47f3db4988b87206836a04b579"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f165d32e7e843e083f63e307b050ec3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0b4d26a47ff443ab5de957a85d53ead"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22aefe5f44d046cd982a0e35cb184462"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b96a0910c1d48b096812b180fee3073"}},"metadata":{}}]},{"cell_type":"markdown","source":"### 3. Dataset\nWe will be using the Stanford Question Answering Dataset (SQuAD 1.1). Download the dataset from [here](https://rajpurkar.github.io/SQuAD-explorer/).\n\n>“The Stanford Question Answering Dataset (SQuAD 1.1) is a popular dataset for training and evaluating question-answering models. It contains more than 100,000 question-answer pairs, each consisting of a question about a passage of text and the corresponding answer. The dataset is widely used in natural language processing research, and is considered to be a benchmark for question-answering performance.”\n>","metadata":{}},{"cell_type":"markdown","source":"Loading the data","metadata":{}},{"cell_type":"code","source":"!mkdir squad\n!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O squad/train-v2.0.json","metadata":{"execution":{"iopub.status.busy":"2024-02-18T16:58:13.592436Z","iopub.execute_input":"2024-02-18T16:58:13.592752Z","iopub.status.idle":"2024-02-18T16:58:15.820993Z","shell.execute_reply.started":"2024-02-18T16:58:13.592724Z","shell.execute_reply":"2024-02-18T16:58:15.819770Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"mkdir: cannot create directory 'squad': File exists\n--2024-02-18 16:58:15--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\nResolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\nConnecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 42123633 (40M) [application/json]\nSaving to: 'squad/train-v2.0.json'\n\nsquad/train-v2.0.js 100%[===================>]  40.17M   230MB/s    in 0.2s    \n\n2024-02-18 16:58:15 (230 MB/s) - 'squad/train-v2.0.json' saved [42123633/42123633]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-02-18T16:58:15.823004Z","iopub.execute_input":"2024-02-18T16:58:15.823340Z","iopub.status.idle":"2024-02-18T16:58:16.802787Z","shell.execute_reply.started":"2024-02-18T16:58:15.823308Z","shell.execute_reply":"2024-02-18T16:58:16.801801Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"squad\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('./squad/train-v2.0.json') as f:\n    data = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T16:59:46.385040Z","iopub.execute_input":"2024-02-18T16:59:46.385387Z","iopub.status.idle":"2024-02-18T16:59:47.279705Z","shell.execute_reply.started":"2024-02-18T16:59:46.385361Z","shell.execute_reply":"2024-02-18T16:59:47.278668Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"The below function `prepare_data` takes in data and extracts the context, questions, and answers from it, and returns the “articles” list containing the extracted context, questions, and answers.","metadata":{}},{"cell_type":"code","source":"def prepare_data(data):\n    articles = []\n    \n    for article in data[\"data\"]:\n        for paragraph in article[\"paragraphs\"]:\n            for qa in paragraph[\"qas\"]:\n                question = qa[\"question\"]\n\n                if not qa[\"is_impossible\"]:\n                    answer = qa[\"answers\"][0][\"text\"]\n                \n                inputs = {\"context\": paragraph[\"context\"], \"question\": question, \"answer\": answer}\n\n            \n                articles.append(inputs)\n\n    return articles","metadata":{"execution":{"iopub.status.busy":"2024-02-18T16:59:48.569452Z","iopub.execute_input":"2024-02-18T16:59:48.570411Z","iopub.status.idle":"2024-02-18T16:59:48.578984Z","shell.execute_reply.started":"2024-02-18T16:59:48.570374Z","shell.execute_reply":"2024-02-18T16:59:48.578198Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"data = prepare_data(data)\nprint(len(data))\n\n# Create a Dataframe\ndata = pd.DataFrame(data)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T16:59:49.947372Z","iopub.execute_input":"2024-02-18T16:59:49.948003Z","iopub.status.idle":"2024-02-18T16:59:50.231554Z","shell.execute_reply.started":"2024-02-18T16:59:49.947966Z","shell.execute_reply":"2024-02-18T16:59:50.230204Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"130319\n","output_type":"stream"}]},{"cell_type":"code","source":"data = data[:50000]","metadata":{"execution":{"iopub.status.busy":"2024-02-18T16:59:58.379276Z","iopub.execute_input":"2024-02-18T16:59:58.379961Z","iopub.status.idle":"2024-02-18T16:59:58.384223Z","shell.execute_reply.started":"2024-02-18T16:59:58.379929Z","shell.execute_reply":"2024-02-18T16:59:58.383222Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"data.head(2)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T17:00:01.112824Z","iopub.execute_input":"2024-02-18T17:00:01.113535Z","iopub.status.idle":"2024-02-18T17:00:01.129420Z","shell.execute_reply.started":"2024-02-18T17:00:01.113506Z","shell.execute_reply":"2024-02-18T17:00:01.128424Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"                                             context  \\\n0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n\n                                            question               answer  \n0           When did Beyonce start becoming popular?    in the late 1990s  \n1  What areas did Beyonce compete in when she was...  singing and dancing  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>context</th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n      <td>When did Beyonce start becoming popular?</td>\n      <td>in the late 1990s</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n      <td>What areas did Beyonce compete in when she was...</td>\n      <td>singing and dancing</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"We will write a custom dataset using the PyTorch framework to make our data task-ready.","metadata":{}},{"cell_type":"code","source":"class QA_Dataset(Dataset):\n    def __init__(self, tokenizer, dataframe, q_len, t_len):\n        self.tokenizer = tokenizer\n        self.q_len = q_len\n        self.t_len = t_len\n        self.data = dataframe\n        self.questions = self.data[\"question\"]\n        self.context = self.data[\"context\"]\n        self.answer = self.data['answer']\n        \n    def __len__(self):\n        return len(self.questions)\n    \n    def __getitem__(self, idx):\n        question = self.questions[idx]\n        context = self.context[idx]\n        answer = self.answer[idx]\n        \n        question_tokenized = self.tokenizer(question, context, max_length=self.q_len, padding=\"max_length\",\n                                                    truncation=True, pad_to_max_length=True, add_special_tokens=True)\n        answer_tokenized = self.tokenizer(answer, max_length=self.t_len, padding=\"max_length\", \n                                          truncation=True, pad_to_max_length=True, add_special_tokens=True)\n        \n        labels = torch.tensor(answer_tokenized[\"input_ids\"], dtype=torch.long)\n        labels[labels == 0] = -100\n        \n        return {\n            \"input_ids\": torch.tensor(question_tokenized[\"input_ids\"], dtype=torch.long),\n            \"attention_mask\": torch.tensor(question_tokenized[\"attention_mask\"], dtype=torch.long),\n            \"labels\": labels,\n            \"decoder_attention_mask\": torch.tensor(answer_tokenized[\"attention_mask\"], dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2024-02-18T17:00:03.816919Z","iopub.execute_input":"2024-02-18T17:00:03.817300Z","iopub.status.idle":"2024-02-18T17:00:03.827388Z","shell.execute_reply.started":"2024-02-18T17:00:03.817270Z","shell.execute_reply":"2024-02-18T17:00:03.826380Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Dataloader\n\ntrain_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n\ntrain_sampler = RandomSampler(train_data.index)\nval_sampler = RandomSampler(val_data.index)\n\nqa_dataset = QA_Dataset(TOKENIZER, data, Q_LEN, T_LEN)\n\ntrain_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\nval_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T17:00:04.398596Z","iopub.execute_input":"2024-02-18T17:00:04.398930Z","iopub.status.idle":"2024-02-18T17:00:04.418962Z","shell.execute_reply.started":"2024-02-18T17:00:04.398906Z","shell.execute_reply":"2024-02-18T17:00:04.418211Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### 4. Training loop","metadata":{}},{"cell_type":"code","source":"train_loss = 0\nval_loss = 0\ntrain_batch_count = 0\nval_batch_count = 0\nEPOCHS = 1\n\nfor epoch in range(EPOCHS):\n    MODEL.train()\n    for batch in tqdm(train_loader, desc=\"Training batches\"):\n        input_ids = batch[\"input_ids\"].to(DEVICE)\n        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n        labels = batch[\"labels\"].to(DEVICE)\n        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n\n        outputs = MODEL(\n                          input_ids=input_ids,\n                          attention_mask=attention_mask,\n                          labels=labels,\n                          decoder_attention_mask=decoder_attention_mask\n                        )\n\n        OPTIMIZER.zero_grad()\n        outputs.loss.backward()\n        OPTIMIZER.step()\n        train_loss += outputs.loss.item()\n        train_batch_count += 1\n    \n    #Evaluation\n    MODEL.eval()\n    for batch in tqdm(val_loader, desc=\"Validation batches\"):\n        input_ids = batch[\"input_ids\"].to(DEVICE)\n        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n        labels = batch[\"labels\"].to(DEVICE)\n        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n\n        outputs = MODEL(\n                          input_ids=input_ids,\n                          attention_mask=attention_mask,\n                          labels=labels,\n                          decoder_attention_mask=decoder_attention_mask\n                        )\n\n        OPTIMIZER.zero_grad()\n        outputs.loss.backward()\n        OPTIMIZER.step()\n        val_loss += outputs.loss.item()\n        val_batch_count += 1\n        \n    print(f\"{epoch+1}/{2} -> Train loss: {train_loss / train_batch_count}\\tValidation loss: {val_loss/val_batch_count}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-18T17:00:09.901224Z","iopub.execute_input":"2024-02-18T17:00:09.901579Z","iopub.status.idle":"2024-02-18T17:29:03.293332Z","shell.execute_reply.started":"2024-02-18T17:00:09.901552Z","shell.execute_reply":"2024-02-18T17:29:03.292368Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Training batches: 100%|██████████| 2500/2500 [23:14<00:00,  1.79it/s]\nValidation batches: 100%|██████████| 625/625 [05:38<00:00,  1.85it/s]","output_type":"stream"},{"name":"stdout","text":"1/2 -> Train loss: 1.0593451853692533\tValidation loss: 0.44639317539930345\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Save model and tokenizer","metadata":{}},{"cell_type":"code","source":"MODEL.save_pretrained(\"qa_t5_model\")\nTOKENIZER.save_pretrained(\"qa_t5_tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2024-02-18T17:29:03.295408Z","iopub.execute_input":"2024-02-18T17:29:03.296092Z","iopub.status.idle":"2024-02-18T17:29:04.925125Z","shell.execute_reply.started":"2024-02-18T17:29:03.296047Z","shell.execute_reply":"2024-02-18T17:29:04.924165Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"('qa_t5_tokenizer/tokenizer_config.json',\n 'qa_t5_tokenizer/special_tokens_map.json',\n 'qa_t5_tokenizer/spiece.model',\n 'qa_t5_tokenizer/added_tokens.json',\n 'qa_t5_tokenizer/tokenizer.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"### 5. Inference\n>BLEU (Bilingual Evaluation Understudy) is a evaluation metric for machine learning models that generate text, such as machine translation and text summarization. It compares the generated text with a reference text and assigns a score between 0 and 1, where 1 is the best score, based on the overlapping n-grams (word sequences) between them. The higher the BLEU score, the more similar the generated text is to the reference text.\n>\nAdditionally, BLEU is not the only metric to evaluate the performance of QA systems, and it’s important to use a set of metrics that are suitable for the task, such as ROUGE, METEOR, METRIC, and others.","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.translate.bleu_score import sentence_bleu\nnltk.download('punkt')\n\ndef calculate_bleu_score(pred, ref):\n    # Tokenize the sentences\n    pred_tokens = nltk.word_tokenize(pred.lower())\n    ref_tokens = nltk.word_tokenize(ref.lower())\n\n    # Calculate BLEU score\n    bleu_score = sentence_bleu([ref_tokens], pred_tokens)\n    return bleu_score","metadata":{"execution":{"iopub.status.busy":"2024-02-18T18:02:31.137375Z","iopub.execute_input":"2024-02-18T18:02:31.137740Z","iopub.status.idle":"2024-02-18T18:02:31.144309Z","shell.execute_reply.started":"2024-02-18T18:02:31.137711Z","shell.execute_reply":"2024-02-18T18:02:31.143224Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"def predict_answer(context, question, ref_answer=None):\n    inputs = TOKENIZER(question, context, max_length=Q_LEN, padding=\"max_length\", truncation=True, add_special_tokens=True)\n    \n    input_ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n    attention_mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n\n    outputs = MODEL.generate(input_ids=input_ids, attention_mask=attention_mask)\n  \n    predicted_answer = TOKENIZER.decode(outputs.flatten(), skip_special_tokens=True)\n    score = calculate_bleu_score(predicted_answer, ref_answer)\n    return {\n            \"context\": context,\n            \"question\": question,\n            \"ref\": ref_answer, \n            \"pred\": predicted_answer, \n            \"score\": score\n        }","metadata":{"execution":{"iopub.status.busy":"2024-02-18T18:03:14.302906Z","iopub.execute_input":"2024-02-18T18:03:14.303775Z","iopub.status.idle":"2024-02-18T18:03:14.311150Z","shell.execute_reply.started":"2024-02-18T18:03:14.303744Z","shell.execute_reply":"2024-02-18T18:03:14.310102Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"Now that we have our inference function, let’s test it on some examples from the same dataset as well as real cases.","metadata":{}},{"cell_type":"code","source":"context = data.iloc[0][\"context\"]\nquestion = data.iloc[0][\"question\"]\nanswer = data.iloc[0][\"answer\"]\n\np = predict_answer(context, question, ref_answer=answer)\nprint(\"Context: \\n\", p['context'])\nprint(\"\\n\")\nprint(\"Question: \\n\", p['question'])\nprint(\"\\n\")\nprint(\"Predicted Ans: \\n\", p['pred'])\nprint(\"\\n\")\nprint(\"Actual Ans: \\n\", p['ref'])\nprint(\"\\n\")\nprint(\"BLEU Score: \\n\", p['score'])","metadata":{"execution":{"iopub.status.busy":"2024-02-18T18:03:29.665689Z","iopub.execute_input":"2024-02-18T18:03:29.666337Z","iopub.status.idle":"2024-02-18T18:03:29.755599Z","shell.execute_reply.started":"2024-02-18T18:03:29.666306Z","shell.execute_reply":"2024-02-18T18:03:29.754673Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"Context: \n Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n\n\nQuestion: \n When did Beyonce start becoming popular?\n\n\nPredicted Ans: \n late 1990s\n\n\nActual Ans: \n in the late 1990s\n\n\nBLEU Score: \n 0.36787944117144233\n","output_type":"stream"}]},{"cell_type":"code","source":"context = data.iloc[56][\"context\"]\nquestion = data.iloc[56][\"question\"]\nanswer = data.iloc[56][\"answer\"]\n\np = predict_answer(context, question, ref_answer=answer)\nprint(\"Context: \\n\", p['context'])\nprint(\"\\n\")\nprint(\"Question: \\n\", p['question'])\nprint(\"\\n\")\nprint(\"Predicted Ans: \\n\", p['pred'])\nprint(\"\\n\")\nprint(\"Actual Ans: \\n\", p['ref'])\nprint(\"\\n\")\nprint(\"BLEU Score: \\n\", p['score'])","metadata":{"execution":{"iopub.status.busy":"2024-02-18T18:03:52.557195Z","iopub.execute_input":"2024-02-18T18:03:52.557529Z","iopub.status.idle":"2024-02-18T18:03:52.725153Z","shell.execute_reply.started":"2024-02-18T18:03:52.557505Z","shell.execute_reply":"2024-02-18T18:03:52.724200Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"Context: \n Beyoncé attended St. Mary's Elementary School in Fredericksburg, Texas, where she enrolled in dance classes. Her singing talent was discovered when dance instructor Darlette Johnson began humming a song and she finished it, able to hit the high-pitched notes. Beyoncé's interest in music and performing continued after winning a school talent show at age seven, singing John Lennon's \"Imagine\" to beat 15/16-year-olds. In fall of 1990, Beyoncé enrolled in Parker Elementary School, a music magnet school in Houston, where she would perform with the school's choir. She also attended the High School for the Performing and Visual Arts and later Alief Elsik High School. Beyoncé was also a member of the choir at St. John's United Methodist Church as a soloist for two years.\n\n\nQuestion: \n I which church was Beyonce  a member and soloist  in the choir?\n\n\nPredicted Ans: \n St. John's United Methodist Church\n\n\nActual Ans: \n St. John's United Methodist Church\n\n\nBLEU Score: \n 1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"context = data.iloc[36][\"context\"]\nquestion = data.iloc[36][\"question\"]\nanswer = data.iloc[36][\"answer\"]\n\np = predict_answer(context, question, ref_answer=answer)\nprint(\"Context: \\n\", p['context'])\nprint(\"\\n\")\nprint(\"Question: \\n\", p['question'])\nprint(\"\\n\")\nprint(\"Predicted Ans: \\n\", p['pred'])\nprint(\"\\n\")\nprint(\"Actual Ans: \\n\", p['ref'])\nprint(\"\\n\")\nprint(\"BLEU Score: \\n\", p['score'])","metadata":{"execution":{"iopub.status.busy":"2024-02-18T18:04:55.513102Z","iopub.execute_input":"2024-02-18T18:04:55.513965Z","iopub.status.idle":"2024-02-18T18:04:55.623211Z","shell.execute_reply.started":"2024-02-18T18:04:55.513932Z","shell.execute_reply":"2024-02-18T18:04:55.622217Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"Context: \n A self-described \"modern-day feminist\", Beyoncé creates songs that are often characterized by themes of love, relationships, and monogamy, as well as female sexuality and empowerment. On stage, her dynamic, highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music. Throughout a career spanning 19 years, she has sold over 118 million records as a solo artist, and a further 60 million with Destiny's Child, making her one of the best-selling music artists of all time. She has won 20 Grammy Awards and is the most nominated woman in the award's history. The Recording Industry Association of America recognized her as the Top Certified Artist in America during the 2000s decade. In 2009, Billboard named her the Top Radio Songs Artist of the Decade, the Top Female Artist of the 2000s and their Artist of the Millennium in 2011. Time listed her among the 100 most influential people in the world in 2013 and 2014. Forbes magazine also listed her as the most powerful female musician of 2015.\n\n\nQuestion: \n After leaving Destiny's Child, how many records did Beyoncé release under her own name?\n\n\nPredicted Ans: \n 118 million\n\n\nActual Ans: \n 118 million\n\n\nBLEU Score: \n 1.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 6. Conclusion\nFinally, this is just a simple example of what can be accomplished with the T5 model, and there are many other creative ways to utilize it in a variety of natural language processing tasks.","metadata":{}}]}